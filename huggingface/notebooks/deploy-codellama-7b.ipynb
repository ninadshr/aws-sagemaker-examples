{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy CodeLlama-7b-Instruct-hf on AWS SageMaker\n",
    "\n",
    "This notebook demonstrates how to deploy Meta's CodeLlama-7b-Instruct-hf model on Amazon SageMaker for manufacturing code generation use cases.\n",
    "\n",
    "## Prerequisites\n",
    "- AWS Account with SageMaker access\n",
    "- Hugging Face account and token\n",
    "- IAM role with SageMaker permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "hub = {\n",
    "    'HF_MODEL_ID': 'codellama/CodeLlama-7b-Instruct-hf',\n",
    "    'HF_TASK': 'text-generation',\n",
    "    'MAX_INPUT_LENGTH': '4096',\n",
    "    'MAX_TOTAL_TOKENS': '8192',\n",
    "}\n",
    "\n",
    "# Create Hugging Face Model\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    transformers_version=\"4.37\",\n",
    "    pytorch_version=\"2.1\",\n",
    "    py_version=\"py310\",\n",
    "    model_server_workers=1,\n",
    ")\n",
    "\n",
    "print(\"Model configuration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    endpoint_name=\"codellama-endpoint\",\n",
    "    container_startup_health_check_timeout=600,\n",
    ")\n",
    "\n",
    "print(f\"Endpoint deployed: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a manufacturing code generation prompt\n",
    "prompt = \"\"\"Write a Python function to control a conveyor belt system with:\n",
    "- Start/Stop functionality\n",
    "- Speed control (0-100%)\n",
    "- Emergency stop\n",
    "- Position tracking\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload)\n",
    "print(\"\\n=== Generated Code ===\")\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Manufacturing Use Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: PLC Ladder Logic\n",
    "plc_prompt = \"\"\"Generate ladder logic for a simple start-stop motor control circuit with:\n",
    "- Start button (normally open)\n",
    "- Stop button (normally closed)\n",
    "- Motor contactor\n",
    "- Overload protection\"\"\"\n",
    "\n",
    "response = predictor.predict({\"inputs\": plc_prompt, \"parameters\": {\"max_new_tokens\": 256}})\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Quality Control Script\n",
    "qc_prompt = \"\"\"Create a Python script for quality control that:\n",
    "1. Measures part dimensions using a digital caliper\n",
    "2. Compares against tolerance specifications\n",
    "3. Generates pass/fail report\n",
    "4. Logs results to CSV file\"\"\"\n",
    "\n",
    "response = predictor.predict({\"inputs\": qc_prompt, \"parameters\": {\"max_new_tokens\": 512}})\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint to avoid charges\n",
    "# predictor.delete_endpoint()\n",
    "# print(\"Endpoint deleted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
